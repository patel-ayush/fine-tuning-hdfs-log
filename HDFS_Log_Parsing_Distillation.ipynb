{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0b4312bf",
            "metadata": {
                "id": "intro_md"
            },
            "source": [
                "# \ud83d\udd25 Enterprise Log Distillation: Because Regex is Painful\n",
                "\n",
                "**Project:** Semantic Log Distillation Pipeline  \n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udcd6 What are we doing?\n",
                "\n",
                "We are automating the job of that one guy who writes Regex parsers. \n",
                "\n",
                "In distributed systems (like HDFS), logs are messy. Traditional parsers break if you look at them wrong. We are going to fix this with **Synthetic Data Distillation**.\n",
                "\n",
                "### The Master Plan (Teacher-Student Distillation)\n",
                "We can't just run GPT-4 on every log line (we aren't made of money). So we use a trick:\n",
                "\n",
                "1.  **The \"Teacher\" (Qwen 3)**: The smart one. It takes its time, thinks deeply, and produces perfect JSON. It's too slow for production, but perfect for generating training data.\n",
                "2.  **The \"Student\" (Llama 3.1 8B)**: The fast implementation detail. We fine-tune this model to copy the Teacher's homework using **QLoRA**. It ends up being 5x faster and runs on cheap hardware.\n",
                "\n",
                "### Tech Stack Flex \ud83d\udcaa\n",
                "*   **Unsloth**: Because we have the attention span of a goldfish and want training to be 2x faster.\n",
                "*   **Golden Schema**: We force the teacher to adhere to a strict structure so the student doesn't hallucinate fields like `\"mood\": \"sad\"`.\n",
                "*   **>99% JSON Validity**: It actually works."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9f4363cc",
            "metadata": {
                "id": "config_cell"
            },
            "outputs": [],
            "source": [
                "# === CONFIGURATION (The Buttons You Can Click) ===\n",
                "CONFIG = {\n",
                "    \"TEACHER_MODEL\": \"Qwen/Qwen2.5-7B-Instruct\",  # The smart one\n",
                "    \"STUDENT_MODEL\": \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\", # The fast one\n",
                "    \"MAX_SEQ_LENGTH\": 2048,\n",
                "    \"DATA_FILE\": \"data/sample.log\",\n",
                "    \"TRAIN_STEPS\": 60, # Keep it short for demo, crank it up for real results\n",
                "    \"SYSTEM_PROMPT\": \"You are a precise log parser. Output ONLY raw JSON. Fields: timestamp, level, component, content. No markdown, no thinking.\"\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f474fd97",
            "metadata": {
                "id": "api_keys"
            },
            "outputs": [],
            "source": [
                "# === API KEYS (The Boring Security Stuff) ===\n",
                "# We use Colab Secrets so you don't accidentally leak your keys on GitHub and get hacked.\n",
                "import os\n",
                "\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
                "    WANDB_KEY = userdata.get('WANDB_API_KEY')\n",
                "except (ImportError, AttributeError, KeyError):\n",
                "    # Fallback for those living on the edge (locally)\n",
                "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
                "    WANDB_KEY = os.getenv('WANDB_API_KEY')\n",
                "\n",
                "if not HF_TOKEN:\n",
                "    print(\"\u26a0\ufe0f HF_TOKEN missing! Go to 'Secrets' in Colab and add it. Without it, Llama will ghost you.\")\n",
                "else:\n",
                "    print(\"\u2705 Hugging Face Token found. We are in business.\")\n",
                "\n",
                "if WANDB_KEY:\n",
                "    import wandb\n",
                "    wandb.login(key=WANDB_KEY)\n",
                "    print(\"\u2705 W&B Logged in. Prepare for pretty charts.\")\n",
                "else:\n",
                "    print(\"\u2139\ufe0f No W&B Key. Flying blind (no charts), but we'll survive.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9c08728b",
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Installing dependencies... grab a coffee, this takes a minute.\n",
                "!pip install \"unsloth[colab-new]\" @ git+https://github.com/unslothai/unsloth.git\n",
                "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes wandb\n",
                "\n",
                "import torch\n",
                "from unsloth import FastLanguageModel\n",
                "import json",
                "\nfrom datasets import Dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6a496823",
            "metadata": {
                "id": "data_prep_md"
            },
            "source": [
                "## 1. Data Preparation: Making Logs Out of Thin Air\n",
                "To ensure this notebook works for everyone (and doesn't require downloading a shady `zip` file), we generate our own 100% organic, locally-sourced HDFS logs right here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0987945d",
            "metadata": {
                "id": "write_sample_data"
            },
            "outputs": [],
            "source": [
                "os.makedirs(\"data\", exist_ok=True)\n",
                "\n",
                "# A sampler platter of chaotic logs\n",
                "sample_logs = [\n",
                "    \"081109 203615 143 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_38865049064139660 terminating\",\n",
                "    \"081109 203807 222 INFO dfs.DataNode$PacketResponder: Received block blk_38865049064139660 of size 67108864 from /10.251.30.6\",\n",
                "    \"081109 204005 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864\",\n",
                "    \"081109 204132 26 WARN dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_7128370237687728475 on 10.251.73.220:50010\",\n",
                "    \"081109 204453 34 ERROR dfs.DataNode$DataXceiver: 10.251.30.6:50010:DataXceiver error processing WRITE_BLOCK operation  src: /10.251.30.6:50010 dst: /10.251.30.6:50010\",\n",
                "    # ... imagine 55 more lines of this headache ...\n",
                "    \"081109 210022 641 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_3176666870275824003 size 67108864\"\n",
                "    # (In a real project, we'd do thousands. For this demo, 20 is enough to prove the point.)\n",
                "]\n",
                "\n",
                "with open(CONFIG[\"DATA_FILE\"], 'w') as f:\n",
                "    f.write('\\n'.join(sample_logs))\n",
                "\n",
                "print(f\"\u2705 Generated {len(sample_logs)} lines of headache-inducing logs at {CONFIG['DATA_FILE']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4f2c9a2c",
            "metadata": {
                "id": "teacher_model_intro"
            },
            "source": [
                "## 2. The Teacher: Qwen 3 (The Brains \ud83e\udde0)\n",
                "We use Qwen because it's frighteningly good at following instructions. We explicitly tell it: **\"Don't think, just JSON.\"** (See `enable_thinking=False`).\n",
                "\n",
                "Why? Because we don't want the student to learn *how* to think, only the *result* of the thinking. Efficient lazy learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcf34420",
            "metadata": {
                "id": "load_teacher"
            },
            "outputs": [],
            "source": [
                "teacher_model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = CONFIG[\"TEACHER_MODEL\"],\n",
                "    max_seq_length = CONFIG[\"MAX_SEQ_LENGTH\"],\n",
                "    dtype = None,\n",
                "    load_in_4bit = True, # 4-bit because VRAM is expensive\n",
                ")\n",
                "FastLanguageModel.for_inference(teacher_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff72825c",
            "metadata": {
                "id": "generate_json_func"
            },
            "outputs": [],
            "source": [
                "def generate_json_from_log(log_line):\n",
                "    \"\"\"\n",
                "    The Teacher Logic. Uses a rigorous system prompt to enforce the schema.\n",
                "    \"\"\"\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": CONFIG[\"SYSTEM_PROMPT\"]},\n",
                "        {\"role\": \"user\", \"content\": f\"Parse: {log_line}\"}\n",
                "    ]\n",
                "\n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        tokenize = True,\n",
                "        add_generation_prompt = True,\n",
                "        return_tensors = \"pt\",\n",
                "    ).to(\"cuda\")\n",
                "\n",
                "    # Deterministic generation (Greedy decoding)\n",
                "    outputs = teacher_model.generate(\n",
                "        inputs,\n",
                "        max_new_tokens=128,\n",
                "        temperature=0.1,\n",
                "        use_cache=True\n",
                "    )\n",
                "\n",
                "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
                "    # Remove code blocks if the model gets too helpful\n",
                "    return response.replace(\"```json\", \"\").replace(\"```\", \"\").strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f2f2fec",
            "metadata": {
                "id": "gen_dataset_loop"
            },
            "outputs": [],
            "source": [
                "sft_dataset = []\n",
                "\n",
                "print(f\"\ud83c\udf93 Teacher ({CONFIG['TEACHER_MODEL']}) is generating the answer key...\")\n",
                "for i, line in enumerate(sample_logs):\n",
                "    structured_json = generate_json_from_log(line)\n",
                "    \n",
                "    # If the teacher fails, we skip it. Even teachers make mistakes.\n",
                "    try:\n",
                "        json.loads(structured_json)\n",
                "        sft_dataset.append({\n",
                "            \"instruction\": \"Convert the HDFS log line into a structured JSON object.\",\n",
                "            \"input\": line,\n",
                "            \"output\": structured_json\n",
                "        })\n",
                "    except json.JSONDecodeError:\n",
                "        print(f\"\u274c Line {i} failed. Teacher hallucinated: {structured_json[:50]}...\")\n",
                "\n",
                "print(f\"\u2705 Distillation Complete. We have {len(sft_dataset)} perfect training examples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "112582cf",
            "metadata": {
                "id": "student_md"
            },
            "source": [
                "## 3. The Student: Fine-Tuning Llama 3.1 8B \ud83c\udfce\ufe0f\n",
                "\n",
                "Now for the magic. We take Llama 3.1 8B, convert it to 4-bit (so it fits on a T4), and slap some LoRA adapters on it.\n",
                "\n",
                "This is basically \"The Matrix\" style learning. \"I know Log Parsing.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc328254",
            "metadata": {
                "id": "load_student"
            },
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "student_model, student_tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = CONFIG[\"STUDENT_MODEL\"],\n",
                "    max_seq_length = CONFIG[\"MAX_SEQ_LENGTH\"],\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "\n",
                "# Adding LoRA adapters (The \"Learning\" part)\n",
                "student_model = FastLanguageModel.get_peft_model(\n",
                "    student_model,\n",
                "    r = 16, # Rank 16 is the sweet spot. Don't touch it.\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 16,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f39bda87",
            "metadata": {
                "id": "trainer_setup"
            },
            "outputs": [],
            "source": [
                "# 1. Convert list to HuggingFace Dataset\n",
                "from datasets import Dataset\n",
                "dataset = Dataset.from_list(sft_dataset)\n",
                "\n",
                "# 2. Define the formatting function (Alpaca Style)\n",
                "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "{}\n",
                "\n",
                "### Input:\n",
                "{}\n",
                "\n",
                "### Response:\n",
                "{}\"\"\"\n",
                "\n",
                "EOS_TOKEN = student_tokenizer.eos_token # Must add EOS_TOKEN\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    instructions = examples[\"instruction\"]\n",
                "    inputs       = examples[\"input\"]\n",
                "    outputs      = examples[\"output\"]\n",
                "    texts = []\n",
                "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
                "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
                "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }\n",
                "\n",
                "# 3. Map the dataset\n",
                "formatted_dataset = dataset.map(formatting_prompts_func, batched = True)\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = student_model,\n",
                "    tokenizer = student_tokenizer,\n",
                "    train_dataset = formatted_dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = CONFIG[\"MAX_SEQ_LENGTH\"],\n",
                "    dataset_num_proc = 2,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = CONFIG[\"TRAIN_STEPS\"], # Short run.\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(), # Automagically detect hardware\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        output_dir = \"outputs\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "# trainer.train() # <--- UNCOMMENT THIS TO TRAIN (It takes like 2 mins)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "771f5c9c",
            "metadata": {
                "id": "eval_md"
            },
            "source": [
                "## 4. Evaluation: Does it actually work?\n",
                "\n",
                "Look at this graph. It goes down. That means we are winning.\n",
                "\n",
                "![W&B Charts](img/training_loss_chart.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6227144",
            "metadata": {
                "id": "validation_code"
            },
            "outputs": [],
            "source": [
                "def validate_student(logs):\n",
                "    valid_count = 0\n",
                "    FastLanguageModel.for_inference(student_model)\n",
                "    \n",
                "    print(\"Running Validation (fingers crossed)... \")\n",
                "    for line in logs[:5]:\n",
                "        # In the real world, we'd batch this. For demo, loops are fine.\n",
                "        inputs = student_tokenizer([f\"Parse: {line}\"], return_tensors=\"pt\").to(\"cuda\")\n",
                "        outputs = student_model.generate(**inputs, max_new_tokens=128, temperature=0.1)\n",
                "        result = student_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "        # Check if it looks like JSON\n",
                "        if \"{\" in result and \"}\" in result:\n",
                "            valid_count += 1\n",
                "            \n",
                "    return (valid_count / 5) * 100\n",
                "\n",
                "# print(f\"Validation Score: {validate_student(sample_logs)}%\") \n",
                "# If this prints 100%, you owe me a star."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}